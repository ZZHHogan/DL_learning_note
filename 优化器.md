### 什么是优化器

优化器或者优化算法，是通过训练优化参数，来最小化（最大化）损失函数。损失函数是用来计算测试集中目标值Y的真实值和预测值的偏差程度。

为了使模型输出逼近或达到最优值，我们需要用各种优化策略和算法，来更新和计算影响模型训练和模型输出的网络参数

### 优化算法的同一框架

首先定义：待优化参数： ![[公式]](https://www.zhihu.com/equation?tex=w) ，目标函数： ![[公式]](https://www.zhihu.com/equation?tex=f%28w%29) ，初始学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)。

然后，开始进行迭代优化。在每个epoch ![[公式]](https://www.zhihu.com/equation?tex=t) ：

1. 计算目标函数关于当前参数的梯度： ![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla+f%28w_t%29)
2. 根据历史梯度计算一阶动量和二阶动量：![[公式]](https://www.zhihu.com/equation?tex=m_t+%3D+%5Cphi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29%3B+V_t+%3D+%5Cpsi%28g_1%2C+g_2%2C+%5Ccdots%2C+g_t%29)，
3. 计算当前时刻的下降梯度： ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta_t+%3D+%5Calpha+%5Ccdot+m_t+%2F+%5Csqrt%7BV_t%7D)
4. 根据下降梯度进行更新： ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta_t)

#### **BGD**

BGD也叫批量梯度下降法，是梯度下降法最常用的形式。学习训练样本的总数为`n`,每次样本`i`为`(x,y)`，模型参数为`w`，代价函数为`J(w)`，每个样本`i`的代价函数关于W的梯度为`ΔJ(w,x,y)`，学习率`η`，更新参数表达式为：

![img](https://upload-images.jianshu.io/upload_images/9060006-41bbad702ef418b0.gif?imageMogr2/auto-orient/strip|imageView2/2/w/268/format/webp)

**优点**：（1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。

​            （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

**缺点**：（1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

​           （2）不能投入新数据实时更新模型。

### SGD

与BGD最大的区别就在于，更新参数的时候，并没有将所有训练样本考虑进去，然后求和除以总数，而是任取一个样本点，然后利用这个样本点进行更新。一开始的SGD没有动量的概念，也就是说梯度下降为：

![img](https://upload-images.jianshu.io/upload_images/9060006-dd7ba301970864aa.gif?imageMogr2/auto-orient/strip|imageView2/2/w/128/format/webp)

![img](https://upload-images.jianshu.io/upload_images/9060006-a36b57e938a0ab62.gif?imageMogr2/auto-orient/strip|imageView2/2/w/293/format/webp)

**优点**：计算梯度快，对于小噪声，SGD可以很好收敛。对于大型数据，训练很快，从数据中取大量的样本算一个梯度，更新一下参数。
**缺点**：噪音较BGD要多，权值更新方向可能出现错误，使得SGD并不是每次迭代都向着整体最优化方向。并且SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。

### SGD with Momentum

为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：

![img](https://upload-images.jianshu.io/upload_images/9060006-20ee6849260f45a2.gif?imageMogr2/auto-orient/strip|imageView2/2/w/245/format/webp)

alpha代表动量大小，一般取为0.9（表示最大速度10倍于SGD）。当前权值的改变受上一次改变的影响，类似加上了惯性。
动量解决SGD的两个问题：1.SGD引入的噪声	2.使网络能更优和更稳定的收敛，减少振荡过程。
当我们将一个小球从山上滚下来时，没有阻力的话，它的动量会越来越大，但是如果遇到了阻力，速度就会变小。加入的这一项，可以使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。
**缺点**：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。

### AdaGrad

AdaGrad使用了二阶动量，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

因此Adagrad方法是通过参数来调整合适的学习率`η`，对稀疏参数（低频）进行大幅更新和对频繁参数（高频）进行小幅更新。

Adagrad缩放每个参数反比于其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应的有快速下降的学习率，而小梯度的参数在学习率上有相对较小的下降。

![img](https:////upload-images.jianshu.io/upload_images/9060006-dac78f40793582fc.png?imageMogr2/auto-orient/strip|imageView2/2/w/343/format/webp)

![img](https:////upload-images.jianshu.io/upload_images/9060006-932c7c13202f2fde?imageMogr2/auto-orient/strip|imageView2/2/w/200/format/webp)



G_t 是个对角矩阵， (i,i) 元素就是 t 时刻参数 θ_i 的梯度平方和。

![img](https:////upload-images.jianshu.io/upload_images/9060006-1c14ac23a2c26252.png?imageMogr2/auto-orient/strip|imageView2/2/w/310/format/webp)

优点：减少了学习率的手动调节，一般 η 就取 0.01。

缺点： 它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小

### RMSprop

由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。

这里修改了AdaGrad的梯度累积为指数加权的移动平均，使在非凸下效果更好。

![img](https:////upload-images.jianshu.io/upload_images/9060006-b6e015e223417ded.png?imageMogr2/auto-orient/strip|imageView2/2/w/306/format/webp)

E[g^2]_t代表前t次的梯度平方的均值。RMSProp的分母取了加权平均，避免学习率越来越低，同时可以自适应调节学习率。

### AdaDelta

这个算法是对 Adagrad 的改进，和 Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，指数衰减平均值

![img](https:////upload-images.jianshu.io/upload_images/9060006-ca59e6671786a169?imageMogr2/auto-orient/strip|imageView2/2/w/285/format/webp)

这个分母相当于梯度的均方根 root mean squared (RMS) ，所以可以用 RMS 简写：

![img](https:////upload-images.jianshu.io/upload_images/9060006-4bb03dcd64d007b0?imageMogr2/auto-orient/strip|imageView2/2/w/288/format/webp)

其中 E 的计算公式如下，t 时刻的依赖于前一时刻的平均和当前的梯度：

![img](https:////upload-images.jianshu.io/upload_images/9060006-b8eb3e38d8dbcff8?imageMogr2/auto-orient/strip|imageView2/2/w/343/format/webp)

梯度更新规则:此外，还将学习率 η 换成了 RMS[Δθ]，这样的话，我们甚至都不需要提前设定学习率了：

![img](https:////upload-images.jianshu.io/upload_images/9060006-d4b0567bc1d08091?imageMogr2/auto-orient/strip|imageView2/2/w/310/format/webp)

评价：在训练的前中期，表现效果较好，加速效果可以，训练速度更快。在后期，模型会反复地在局部最小值附近抖动。

### Adam

我们看到，SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来：Adaptive + Momentum，就是Adam了。

除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 v_t 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 m_t 的指数衰减平均值：

![img](https:////upload-images.jianshu.io/upload_images/9060006-9fdcc1e7c1003c9a.png?imageMogr2/auto-orient/strip|imageView2/2/w/342/format/webp)

如果 m_t 和 v_t 被初始化为 0 向量，那它们就会向 0 偏置，所以做了偏差校正，通过计算偏差校正后的 mt 和 vt 来抵消这些偏差：

![img](https:////upload-images.jianshu.io/upload_images/9060006-794c3a73122a90c7.png?imageMogr2/auto-orient/strip|imageView2/2/w/230/format/webp)

梯度更新规则:

![img](https:////upload-images.jianshu.io/upload_images/9060006-b5523f21e81d7190.png?imageMogr2/auto-orient/strip|imageView2/2/w/285/format/webp)



